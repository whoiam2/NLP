{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Solution from Stephen Thomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.24.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\w.sun\\\\Downloads'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        5000 non-null   int64 \n",
      " 1   sms_text  5000 non-null   object\n",
      " 2   spam      5000 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 117.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sms_text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sms_text  spam\n",
       "0   1  Hope you are having a good week. Just checking in     0\n",
       "1   2                            K..give back my thanks.     0\n",
       "2   3        Am also doing in cbe only. But have to pay.     0\n",
       "3   4  complimentary 4 STAR Ibiza Holiday or £10,000 ...     1\n",
       "4   5  okmail: Dear Dave this is your final notice to...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spamraw_train.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['sms_text']\n",
    "y = df['spam']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.005, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Functions for Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import unidecode\n",
    "import textstat\n",
    "import string  \n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# Simple preprocessor.\n",
    "# Input is a single document, as a single string.\n",
    "# Otuput should be a single document, as a single string.\n",
    "def my_preprocess(doc):\n",
    "    \n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Replace URL with URL string\n",
    "    doc = re.sub(r'http\\S+', 'URL', doc)\n",
    "    \n",
    "    # Replace AT with AT string\n",
    "    doc = re.sub(r'@', 'AT', doc)\n",
    "    \n",
    "    # Replace all numbers/digits with the string NUM\n",
    "    doc = re.sub(r'\\b\\d+\\b', 'NUM', doc)\n",
    "    \n",
    "    # Lemmatize each word.\n",
    "    doc = ' '.join([lemmer.lemmatize(w) for w in doc.split()])\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions will calculate additional features on the document.\n",
    "# They will be put into the Pipeline, called via the FunctionTransformer() function.\n",
    "# Each one takes an entier corpus (as a list of documents), and should return\n",
    "# an array of feature values (one for each document in the corpus).\n",
    "# These functions can do anything they want; I've made most of them quick\n",
    "# one-liners Hopefully the names of the functions will make them self explanitory.\n",
    "\n",
    "def doc_length(corpus):\n",
    "    return np.array([len(doc) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def lexicon_count(corpus):\n",
    "    return np.array([textstat.lexicon_count(doc) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def _get_punc(doc):\n",
    "    return len([a for a in doc if a in string.punctuation])\n",
    "\n",
    "def punc_count(corpus):\n",
    "    return np.array([_get_punc(doc) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def _get_caps(doc):\n",
    "    return sum([1 for a in doc if a.isupper()])\n",
    "\n",
    "def capital_count(corpus):\n",
    "    return np.array([_get_caps(doc) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def num_exclamation_marks(corpus):\n",
    "    return np.array([doc.count('!') for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def num_question_marks(corpus):\n",
    "    return np.array([doc.count('?') for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def xxx_pics_count(corpus):\n",
    "    return np.array([\"xxx pics\" in doc.lower() for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "# See if the document ends with someting like \"Love Steve XXX\"\n",
    "def has_lovexxx(corpus):\n",
    "    return np.array([bool(re.search(r\"l[ou]+ve?.{0,10}x{2,5}\\.? ?$\", doc.lower())) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def has_url(corpus):\n",
    "    return np.array([bool(re.search(\"http\", doc.lower())) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def has_pence(corpus):\n",
    "    return np.array([bool(re.search(\"\\dp\\W\", doc.lower())) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def has_money(corpus):\n",
    "    return np.array([bool(re.search(\"[\\$£]|\\bpence\\b|\\bdollar\\b\", doc.lower())) for doc in corpus]).reshape(-1, 1)\n",
    "\n",
    "def has_sexy_phrase(corpus):\n",
    "    return np.array([bool(re.search(\"sexy single|\\bfree sexy\\b|\\bsexy pic\\b|\\blive sex\\b\", doc.lower())) for doc in corpus]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for class 0: 0.58\n",
      "Weight for class 1: 3.71\n"
     ]
    }
   ],
   "source": [
    "# To help handle class imbalance, calculate the class weights.\n",
    "\n",
    "import numpy as np\n",
    "neg, pos = np.bincount(df['spam'])\n",
    "total = neg + pos\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from sklearn.feature_extraction import stop_words\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Need to preprocess the stopwords, because scikit learn's TfidfVectorizer\n",
    "# removes stopwords _after_ preprocessing\n",
    "#stop_words = [my_preprocess(word) for word in stop_words.ENGLISH_STOP_WORDS]\n",
    "\n",
    "# This vectorizer will be used to create the BOW features\n",
    "vectorizer = TfidfVectorizer(preprocessor=my_preprocess, \n",
    "                             max_features = 1000, \n",
    "                             ngram_range=[1,4],\n",
    "                             stop_words=None,\n",
    "                             strip_accents=\"unicode\", \n",
    "                             lowercase=False, max_df=0.25, min_df=0.001, use_idf=True)\n",
    "\n",
    "# This vectorizer will be used to preprocess the text before topic modeling.\n",
    "# (I _could_ use the same vectorizer as above- but why limit myself?)\n",
    "vectorizer2 = TfidfVectorizer(preprocessor=my_preprocess, \n",
    "                             max_features = 1000, \n",
    "                             ngram_range=[1,2],\n",
    "                             stop_words=None,\n",
    "                             strip_accents=\"unicode\", \n",
    "                             lowercase=False, max_df=0.25, min_df=0.001, use_idf=True)\n",
    "\n",
    "nmf = NMF(n_components=25, random_state=1, init='nndsvda', solver='mu', alpha=.1, l1_ratio=.5)\n",
    "rf = RandomForestClassifier(criterion='entropy', random_state=223)\n",
    "mlp = MLPClassifier(random_state=42, verbose=2, max_iter=200)\n",
    "\n",
    "\n",
    "\n",
    "feature_processing =  FeatureUnion([ \n",
    "    ('bow', Pipeline([('cv', vectorizer), ])),\n",
    "    ('topics', Pipeline([('cv', vectorizer2), ('nmf', nmf),])),\n",
    "    ('length', FunctionTransformer(doc_length, validate=False)),\n",
    "    ('words', FunctionTransformer(lexicon_count, validate=False)),\n",
    "    ('punc_count', FunctionTransformer(punc_count, validate=False)),\n",
    "    ('capital_count', FunctionTransformer(capital_count, validate=False)),  \n",
    "    ('num_exclamation_marks', FunctionTransformer(num_exclamation_marks, validate=False)),  \n",
    "    ('num_question_marks', FunctionTransformer(num_question_marks, validate=False)),  \n",
    "    ('xxx_pics_count', FunctionTransformer(xxx_pics_count, validate=False)),  \n",
    "    ('has_lovexxx', FunctionTransformer(has_lovexxx, validate=False)),  \n",
    "    ('has_url', FunctionTransformer(has_url, validate=False)),  \n",
    "    ('has_pence', FunctionTransformer(has_pence, validate=False)),  \n",
    "    ('has_money', FunctionTransformer(has_money, validate=False)),\n",
    "    ('has_sexy_phrase', FunctionTransformer(has_sexy_phrase, validate=False)),\n",
    "])\n",
    "\n",
    "steps = [('features', feature_processing)]\n",
    "\n",
    "pipe = Pipeline([('features', feature_processing), ('clf', mlp)])\n",
    "\n",
    "param_grid = {}\n",
    "\n",
    "# You - yes you! Manually choose which classifier run you'd like to try.\n",
    "# In future I'd like to automate this so that both are tried; but for this simple\n",
    "# Kaggle competition, I'm keeping it simple. You can set this to either:\n",
    "#\n",
    "# \"RF\" - Random Forest\n",
    "# \"MLP\" - NN\n",
    "#\n",
    "# and then re-run the entire notebook\n",
    "which_clf = \"RF\"\n",
    "\n",
    "if which_clf == \"RF\":\n",
    "\n",
    "    steps.append(('clf', rf))\n",
    "\n",
    "    # I already ran a 4-hour extensive grid; this is not the full set. BTW, the best hyperarms I found are:\n",
    "    # Best parameter (CV scy_train0.988):\n",
    "    # {'clf__class_weight': None, \n",
    "    # 'clf__n_estimators': 500, \n",
    "    # 'features__bow__cv__max_features': 500, \n",
    "    # 'features__bow__cv__preprocessor': None, \n",
    "    # 'features__bow__cv__use_idf': False, \n",
    "    # 'features__topics__cv__stop_words': None, \n",
    "    # 'features__topics__nmf__n_components': 300}\n",
    "    param_grid = {\n",
    "        'features__bow__cv__preprocessor': [None, my_preprocess],\n",
    "        'features__bow__cv__max_features': [50, 200, 500, 1000, 5000],\n",
    "        'features__bow__cv__use_idf': [True, False],\n",
    "        'features__topics__cv__stop_words': [None],\n",
    "        'features__topics__nmf__n_components': [25, 75, 150],\n",
    "        'clf__n_estimators': [100, 500],\n",
    "        'clf__class_weight': [None, class_weight],\n",
    "    }\n",
    "    \n",
    "elif which_clf == \"MLP\":\n",
    "    \n",
    "    steps.append(('clf', mlp))\n",
    "\n",
    "    # I already ran a 4-hour extensive grid; this is not the full set. BTW, the best hyperarms I found are:\n",
    "    # Best parameter (CV scy_train0.991): \n",
    "    # {'clf__hidden_layer_sizes': (25, 25, 25), \n",
    "    # 'features__bow__cv__max_features': 3000, \n",
    "    # 'features__bow__cv__min_df': 0, \n",
    "    # 'features__bow__cv__preprocessor': <function my_preprocess at 0x0000024801E161E0>, \n",
    "    # 'features__bow__cv__use_idf': False, \n",
    "    # 'features__topics__nmf__n_components': 300}\n",
    "    param_grid = {\n",
    "        'features__bow__cv__preprocessor': [my_preprocess],\n",
    "        'features__bow__cv__max_features': [1000, 3000],\n",
    "        'features__bow__cv__min_df': [0],\n",
    "        'features__bow__cv__use_idf': [False],\n",
    "        'features__topics__nmf__n_components': [300],\n",
    "        'clf__hidden_layer_sizes': [(100, ), (50, 50), (25, 25, 25)],\n",
    "    }\n",
    "\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3, scoring='f1_micro', return_train_score=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model\n",
    "\n",
    "It's showtime, baby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n"
     ]
    }
   ],
   "source": [
    "search = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV scy_train0.988):\n",
      "{'clf__class_weight': None, 'clf__n_estimators': 100, 'features__bow__cv__max_features': 1000, 'features__bow__cv__preprocessor': None, 'features__bow__cv__use_idf': False, 'features__topics__cv__stop_words': None, 'features__topics__nmf__n_components': 150}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameter (CV scy_train%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clf__class_weight</th>\n",
       "      <th>clf__n_estimators</th>\n",
       "      <th>features__bow__cv__max_features</th>\n",
       "      <th>features__bow__cv__preprocessor</th>\n",
       "      <th>features__bow__cv__use_idf</th>\n",
       "      <th>features__topics__cv__stop_words</th>\n",
       "      <th>features__topics__nmf__n_components</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>24.535563</td>\n",
       "      <td>2.137078</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.988200</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>&lt;function my_preprocess at 0x000001A69E265E50&gt;</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>26.523727</td>\n",
       "      <td>1.858391</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>27.745744</td>\n",
       "      <td>1.694011</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>None</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>25.625283</td>\n",
       "      <td>1.594532</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>7.890456</td>\n",
       "      <td>2.593838</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.987200</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>4.883458</td>\n",
       "      <td>0.921526</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.980599</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>4.932565</td>\n",
       "      <td>0.762892</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.980400</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>{0: 0.5777675063554426, 1: 3.714710252600297}</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>18.535887</td>\n",
       "      <td>2.292993</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.980399</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>{0: 0.5777675063554426, 1: 3.714710252600297}</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>16.825491</td>\n",
       "      <td>3.169034</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.980199</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>{0: 0.5777675063554426, 1: 3.714710252600297}</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>18.810427</td>\n",
       "      <td>3.441409</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.979599</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 clf__class_weight  clf__n_estimators  \\\n",
       "41                                            None                100   \n",
       "95                                            None                500   \n",
       "98                                            None                500   \n",
       "101                                           None                500   \n",
       "37                                            None                100   \n",
       "..                                             ...                ...   \n",
       "48                                            None                100   \n",
       "3                                             None                100   \n",
       "231  {0: 0.5777675063554426, 1: 3.714710252600297}                500   \n",
       "171  {0: 0.5777675063554426, 1: 3.714710252600297}                100   \n",
       "168  {0: 0.5777675063554426, 1: 3.714710252600297}                100   \n",
       "\n",
       "     features__bow__cv__max_features  \\\n",
       "41                              1000   \n",
       "95                               500   \n",
       "98                              1000   \n",
       "101                             1000   \n",
       "37                              1000   \n",
       "..                               ...   \n",
       "48                              5000   \n",
       "3                                 50   \n",
       "231                             5000   \n",
       "171                             5000   \n",
       "168                             5000   \n",
       "\n",
       "                    features__bow__cv__preprocessor  \\\n",
       "41                                             None   \n",
       "95   <function my_preprocess at 0x000001A69E265E50>   \n",
       "98                                             None   \n",
       "101                                            None   \n",
       "37                                             None   \n",
       "..                                              ...   \n",
       "48                                             None   \n",
       "3                                              None   \n",
       "231                                            None   \n",
       "171                                            None   \n",
       "168                                            None   \n",
       "\n",
       "     features__bow__cv__use_idf features__topics__cv__stop_words  \\\n",
       "41                        False                             None   \n",
       "95                        False                             None   \n",
       "98                         True                             None   \n",
       "101                       False                             None   \n",
       "37                         True                             None   \n",
       "..                          ...                              ...   \n",
       "48                         True                             None   \n",
       "3                         False                             None   \n",
       "231                       False                             None   \n",
       "171                       False                             None   \n",
       "168                        True                             None   \n",
       "\n",
       "     features__topics__nmf__n_components  mean_fit_time  mean_score_time  \\\n",
       "41                                   150      24.535563         2.137078   \n",
       "95                                   150      26.523727         1.858391   \n",
       "98                                   150      27.745744         1.694011   \n",
       "101                                  150      25.625283         1.594532   \n",
       "37                                    75       7.890456         2.593838   \n",
       "..                                   ...            ...              ...   \n",
       "48                                    25       4.883458         0.921526   \n",
       "3                                     25       4.932565         0.762892   \n",
       "231                                   25      18.535887         2.292993   \n",
       "171                                   25      16.825491         3.169034   \n",
       "168                                   25      18.810427         3.441409   \n",
       "\n",
       "     mean_train_score  std_train_score  mean_test_score  std_test_score  \\\n",
       "41             0.9999         0.000141         0.988200        0.001418   \n",
       "95             1.0000         0.000000         0.987400        0.002137   \n",
       "98             1.0000         0.000000         0.987400        0.001769   \n",
       "101            1.0000         0.000000         0.987400        0.001769   \n",
       "37             0.9999         0.000141         0.987200        0.000748   \n",
       "..                ...              ...              ...             ...   \n",
       "48             0.9993         0.000283         0.980599        0.002214   \n",
       "3              0.9985         0.000245         0.980400        0.001725   \n",
       "231            0.9993         0.000374         0.980399        0.002998   \n",
       "171            0.9992         0.000141         0.980199        0.004191   \n",
       "168            0.9987         0.000374         0.979599        0.003066   \n",
       "\n",
       "     rank_test_score  \n",
       "41                 1  \n",
       "95                 2  \n",
       "98                 3  \n",
       "101                3  \n",
       "37                 5  \n",
       "..               ...  \n",
       "48               236  \n",
       "3                237  \n",
       "231              238  \n",
       "171              239  \n",
       "168              240  \n",
       "\n",
       "[240 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the results of hyperparmater tuning\n",
    "\n",
    "def cv_results_to_df(cv_results):\n",
    "    results = pd.DataFrame(list(cv_results['params']))\n",
    "    results['mean_fit_time'] = cv_results['mean_fit_time']\n",
    "    results['mean_score_time'] = cv_results['mean_score_time']\n",
    "    results['mean_train_score'] = cv_results['mean_train_score']\n",
    "    results['std_train_score'] = cv_results['std_train_score']\n",
    "    results['mean_test_score'] = cv_results['mean_test_score']\n",
    "    results['std_test_score'] = cv_results['std_test_score']\n",
    "    results['rank_test_score'] = cv_results['rank_test_score']\n",
    "\n",
    "    results = results.sort_values(['mean_test_score'], ascending=False)\n",
    "    return results\n",
    "\n",
    "results = cv_results_to_df(search.cv_results_)\n",
    "results\n",
    "#results.to_csv('results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Model Performance on Val Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we are using a pipeline and a GridSearchCV, things are a bit complicated.\n",
    "# I want to get references to the objects from the pipeline with the *best* hyperparameter settings,\n",
    "# so that I can explore those objects (later). \n",
    "# The code below is a bit ugly, but after reading throught the docs of Pipeline, \n",
    "# I believe this is the only way to do it.\n",
    "\n",
    "# The pipeline with the best performance\n",
    "pipeline = search.best_estimator_\n",
    "\n",
    "# Get the feature processing pipeline, so I can use it later\n",
    "feature_processing_obj = pipeline.named_steps['features']\n",
    "\n",
    "# Find the vectorizer objects, the NMF objects, and the classifier objects\n",
    "pipevect= dict(pipeline.named_steps['features'].transformer_list)\n",
    "vectorizer_obj = pipevect.get('bow').named_steps['cv']\n",
    "vectorizer_obj2 = pipevect.get('topics').named_steps['cv']\n",
    "nmf_obj = pipevect.get('topics').named_steps['nmf']\n",
    "clf_obj = pipeline.named_steps['clf']\n",
    "\n",
    "# Sanity check - what was vocabSize set to? Should match the output here.\n",
    "len(vectorizer_obj.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[20  0]\n",
      " [ 0  5]]\n",
      "\n",
      "F1 Score = 1.00000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "features_val = feature_processing_obj.transform(X_val).todense()\n",
    "\n",
    "pred_val = search.predict(X_val)\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_val, pred_val))\n",
    "\n",
    "print(\"\\nF1 Score = {:.5f}\".format(f1_score(y_val, pred_val, average='macro')))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Performance on Test\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File sms-spam/spamraw_test_solutions.csv does not exist: 'sms-spam/spamraw_test_solutions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-26657ab2fc16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#my_submission.to_csv('steve_submission.csv', index=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msolutions_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sms-spam/spamraw_test_solutions.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolutions_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File sms-spam/spamraw_test_solutions.csv does not exist: 'sms-spam/spamraw_test_solutions.csv'"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('spamraw_test.csv')\n",
    "\n",
    "features_test = feature_processing_obj.transform(test_df['sms_text']).todense()\n",
    "pred_test = search.predict(test_df['sms_text'])\n",
    "\n",
    "# Output the predictions to a file to upload to Kaggle.\n",
    "# Uncomment to actually create the file\n",
    "#my_submission = pd.DataFrame({'id': test_df.id, 'predicted': pred_test})\n",
    "#my_submission.to_csv('steve_submission.csv', index=False)\n",
    "\n",
    "solutions_df = pd.read_csv('sms-spam/spamraw_test_solutions.csv')\n",
    "y_test = solutions_df['spam']\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, pred_test))\n",
    "\n",
    "print(\"\\nF1 Score = {:.5f}\".format(f1_score(y_test, pred_test, average=\"macro\")))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Model Further\n",
    "\n",
    "The path to enlightment begins by understanding what our model learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Topics\n",
    "\n",
    "Print the top words for each of the NMF topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "def get_top_words(H, feature_names):\n",
    "    output = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_words = [(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        output.append(top_words)\n",
    "        \n",
    "    return pd.DataFrame(output) \n",
    "\n",
    "top_words = get_top_words(nmf_obj.components_, vectorizer_obj2.get_feature_names())\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Feature Importances\n",
    "\n",
    "Note: this section will only work with models that have `.feature_importances_`, such as RF and DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_feature_names = [\"topic {}\".format(i) for i in range(nmf_obj.n_components_)]\n",
    "\n",
    "stat_feature_names = [t[0] for t in pipeline.named_steps['features'].transformer_list if t[0] not in ['topics', 'bow']]\n",
    "\n",
    "feature_names = vectorizer_obj.get_feature_names() + topic_feature_names + stat_feature_names\n",
    "len(feature_names)\n",
    "\n",
    "feature_importances = None\n",
    "if hasattr(clf_obj, 'feature_importances_'):\n",
    "    feature_importances = clf_obj.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = feature_processing_obj.transform(X_train).todense()\n",
    "\n",
    "if feature_importances is None:\n",
    "    print(\"No Feature importances! Skipping.\")\n",
    "else:\n",
    "    N = features_train.shape[1]\n",
    "\n",
    "    ssum = np.zeros(N)\n",
    "    avg = np.zeros(N)\n",
    "    avg_spam = np.zeros(N)\n",
    "    avg_ham = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        ssum[i] = sum(features_train[:, i]).reshape(-1, 1)\n",
    "        avg[i] = np.mean(features_train[:, i]).reshape(-1, 1)\n",
    "        avg_spam[i] = np.mean(features_train[y_train==1, i]).reshape(-1, 1)\n",
    "        avg_ham[i] = np.mean(features_train[y_train==0, i]).reshape(-1, 1)\n",
    "\n",
    "    rf = search.best_estimator_\n",
    "    imp = pd.DataFrame(data={'feature': feature_names, 'imp': feature_importances, 'sum': ssum, 'avg': avg, 'avg_ham': avg_ham, 'avg_spam': avg_spam})\n",
    "    imp = imp.sort_values(by='imp', ascending=False)\n",
    "    imp.head(20)\n",
    "    imp.tail(10)\n",
    "    #imp.to_csv('importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further explanation on Val Data\n",
    "\n",
    "This cool package will explain all the predictions of a tree-based model. I'll have it explain all predictions that were incorrect, to see what is going on (and hopefully inform some additional feature engineering or cleaning steps).\n",
    "\n",
    "Note: this only works on tree-based models, like RF. This cell will crash when using, e.g., MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_importances is None:\n",
    "    print(\"No Feature importances! Skipping.\")\n",
    "else:\n",
    "\n",
    "    from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "    prediction, bias, contributions = ti.predict(clf_obj, features_val)\n",
    "\n",
    "    for i in range(len(features_val)):\n",
    "        if y_val.iloc[i] == pred_val[i]:\n",
    "            continue\n",
    "        print(\"Instance {}\".format(i))\n",
    "        X_val.iloc[i]\n",
    "        print(\"Bias (trainset mean) {}\".format(bias[i]))\n",
    "        print(\"Truth {}\".format(y_val.iloc[i]))\n",
    "        print(\"Prediction {}\".format(prediction[i, :]))\n",
    "        print(\"Feature contributions:\")\n",
    "        con = pd.DataFrame(data={'feature': feature_names, \n",
    "                                 'value': features_val[i].A1,\n",
    "                                 'legit contr': contributions[i][:, 0],\n",
    "                                 'spam contr': contributions[i][:, 1],\n",
    "                                 'abs contr': abs(contributions[i][:, 1])})\n",
    "\n",
    "        con = con.sort_values(by=\"abs contr\", ascending=False)\n",
    "        con['spam cumulative'] = con['spam contr'].cumsum() + bias[i][1]\n",
    "        con.head(30)\n",
    "        print(\"-\"*20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if  feature_importances is None:\n",
    "    print(\"No Feature importances! Skipping.\")\n",
    "else:\n",
    "\n",
    "    from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "    prediction, bias, contributions = ti.predict(clf_obj, features_test)\n",
    "\n",
    "    for i in range(len(features_test)):\n",
    "        if y_test[i] == pred_test[i]:\n",
    "            continue\n",
    "        print(\"Instance {}\".format(i))\n",
    "        test_df.iloc[i, :].sms_text\n",
    "        print(\"Bias (trainset mean) {}\".format(bias[i]))\n",
    "        print(\"Truth {}\".format(y_test[i]))\n",
    "        print(\"Prediction {}\".format(prediction[i, :]))\n",
    "        print(\"Feature contributions:\")\n",
    "        con = pd.DataFrame(data={'feature': feature_names,\n",
    "                                 'value': features_test[i].A1,\n",
    "                                 'legit contr': contributions[i][:, 0],\n",
    "                                 'spam contr': contributions[i][:, 1],\n",
    "                                 'abs contr': abs(contributions[i][:, 1])})\n",
    "        con = con.sort_values(by=\"abs contr\", ascending=False)\n",
    "        con['spam cumulative'] = con['spam contr'].cumsum() + bias[i][1]\n",
    "        con.head(30)\n",
    "        print(\"-\"*20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
